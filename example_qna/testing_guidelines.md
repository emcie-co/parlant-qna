# How do you test guidelines effectively?

Testing guidelines requires a different approach from traditional software testing because we're dealing with natural language interactions. The key is to combine structured test cases with exploratory testing.

Start with your core user journeys. Create test conversations that represent typical interactions, and verify that your agent handles them correctly. But don't stop there - also test edge cases and combinations of conditions. What happens when a premium customer asks about a basic feature? How does the agent handle a request that triggers multiple guidelines?

Parlant allows you to see which guidelines were activated and why, helping you verify that your agent's behavior aligns with your intentions. This makes it possible to catch potential issues before they affect real users.

Most importantly, you can test changes in isolation. When adding new guidelines, you can verify they work as intended without worrying about breaking existing behavior, because Parlant automatically checks for conflicts.
